#!/usr/bin/env python

import os
from threading import RLock
import torch
import numpy as np
import rospy
from cv_bridge import CvBridge
from grid_map_msgs.msg import GridMap
from monoforce.config import Config
from monoforce.models import RigidBodySoftTerrain, State
from monoforce.models.lss.model import compile_model
from monoforce.models.lss.tools import img_transform
from monoforce.ros import height_map_to_point_cloud_msg, height_map_to_gridmap_msg, to_marker, to_path
from monoforce.utils import normalize
from nav_msgs.msg import Path
from sensor_msgs.msg import Image, CompressedImage, PointCloud2, CameraInfo
import rospkg
from time import time
from message_filters import ApproximateTimeSynchronizer, Subscriber
from visualization_msgs.msg import MarkerArray
import tf2_ros
from PIL import Image as PILImage
from ros_numpy import numpify
from collections import OrderedDict


pkg_path = rospkg.RosPack().get_path('monoforce')
torch.set_default_dtype(torch.float32)

class LSS:
    def __init__(self, cfg: Config,
                 data_aug_conf: dict,
                 grid_conf: dict,
                 model_weights,
                 height_map_frame='base_link',
                 img_topics=['/camera_front/color/image_raw/compressed',
                             '/camera_left/color/image_raw/compressed',
                             '/camera_right/color/image_raw/compressed',
                             '/camera_rear/color/image_raw/compressed'],
                 camera_info_topics=['/camera_front/color/camera_info',
                                     '/camera_left/color/camera_info',
                                     '/camera_right/color/camera_info',
                                     '/camera_rear/color/camera_info'],
                 img_mean=None,
                 img_std=None,
                 linear_vels=[1.],
                 angular_vels=[0.]):
        self.cfg = cfg
        self.data_aug_conf = data_aug_conf
        self.grid_conf = grid_conf

        self.model = self.load_model(model_weights)

        self.height_map_frame = height_map_frame

        self.img_topics = img_topics
        self.camera_info_topics = camera_info_topics
        self.img_mean = img_mean
        self.img_std = img_std

        self.linear_vels = linear_vels
        self.angular_vels = angular_vels

        # cv bridge
        self.cv_bridge = CvBridge()
        # tf listener
        self.tf_buffer = tf2_ros.Buffer()
        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer)
        # height map publisher
        self.hm_img_pub = rospy.Publisher('/height_map/image', Image, queue_size=1)
        # point cloud publisher
        self.hm_cloud_pub = rospy.Publisher('/height_map/points', PointCloud2, queue_size=1)
        # grid map publisher
        self.hm_grid_pub = rospy.Publisher('/grid_map', GridMap, queue_size=1)
        # paths publisher
        self.paths_pub = rospy.Publisher('/sampled_paths', MarkerArray, queue_size=1)
        # lower cost path publisher
        self.lc_path_pub = rospy.Publisher('/lower_cost_path', Path, queue_size=1)

        # subscribe to camera intrinsics (ones)
        self.num_cameras = self.data_aug_conf['Ncams']
        self.cam_info_lock = RLock()
        self.cams_intrins = {}
        self.cams_to_hm = {}
        assert self.num_cameras == len(img_topics)
        self.cam_info_subs = [rospy.Subscriber(self.camera_info_topics[i], CameraInfo,
                                               lambda msg, i=i: self.get_cam_calib(msg, i), queue_size=2)
                              for i in range(self.num_cameras)]

        # subscribe to images with approximate time synchronization
        self.img_subs = []
        for img_topic in img_topics:
            rospy.loginfo('Subscribing to %s' % img_topic)
            self.img_subs.append(Subscriber(img_topic, CompressedImage))
        self.img_sync = ApproximateTimeSynchronizer(self.img_subs, queue_size=1, slop=0.1)
        self.img_sync.registerCallback(self.imgs_callback)

    def load_model(self, modelf):
        rospy.loginfo('Loading model %s' % modelf)
        model = compile_model(self.grid_conf, self.data_aug_conf, outC=1)
        model.load_state_dict(torch.load(modelf))
        model.to(self.cfg.device)
        model.eval()
        return model

    def preprocess_params(self):
        H, W = self.data_aug_conf['H'], self.data_aug_conf['W']
        fH, fW = self.data_aug_conf['final_dim']
        resize = max(fH/H, fW/W)
        resize_dims = (int(W*resize), int(H*resize))
        newW, newH = resize_dims
        crop_h = int((1 - np.mean(self.data_aug_conf['bot_pct_lim']))*newH) - fH
        crop_w = int(max(0, newW - fW) / 2)
        crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)
        return resize, resize_dims, crop

    def standardize_img(self, img):
        H, W, C = img.shape
        img_01 = normalize(img)
        img_norm = (img_01 - self.img_mean.reshape((1, 1, C))) / self.img_std.reshape((1, 1, C))
        return img_norm

    def preprocess_img(self, img):
        post_rot = torch.eye(2)
        post_tran = torch.zeros(2)

        # preprocessing parameters (resize, crop)
        resize, resize_dims, crop = self.preprocess_params()
        img, post_rot2, post_tran2 = img_transform(PILImage.fromarray(img), post_rot, post_tran,
                                                   resize=resize,
                                                   resize_dims=resize_dims,
                                                   crop=crop,
                                                   flip=False,
                                                   rotate=0)

        # for convenience, make augmentation matrices 3x3
        post_tran = torch.zeros(3, dtype=torch.float32)
        post_rot = torch.eye(3, dtype=torch.float32)
        post_tran[:2] = post_tran2
        post_rot[:2, :2] = post_rot2

        img = self.standardize_img(np.asarray(img))
        img = torch.as_tensor(img, dtype=torch.float32).permute((2, 0, 1))

        return img, post_rot, post_tran

    def get_cam_calib(self, msg, i):
        """Store camera calibration for i-th camera."""
        assert isinstance(msg, CameraInfo)
        assert isinstance(i, int)

        time = rospy.Time(0)
        timeout = rospy.Duration.from_sec(1.0)
        try:
            tf = self.tf_buffer.lookup_transform(self.height_map_frame, msg.header.frame_id, time, timeout)
        except tf2_ros.TransformException as ex:
            rospy.logerr('Could not transform from camera %s to robot %s: %s.',
                         msg.header.frame_id, self.height_map_frame, ex)
            return

        tf = torch.tensor(numpify(tf.transform), dtype=torch.float32)
        assert isinstance(tf, torch.Tensor)
        with self.cam_info_lock:
            rospy.loginfo('Got calibration for camera %i (%s).', i, msg.header.frame_id)
            self.cams_intrins[msg.header.frame_id] = torch.as_tensor(msg.K, dtype=torch.float32).view(3, 3)
            self.cams_to_hm[msg.header.frame_id] = tf
            self.cam_info_subs[i].unregister()
            rospy.logwarn('Camera %i (%s) unsubscribed.', i, msg.header.frame_id)

    def imgs_callback(self, msg1, msg2, msg3, msg4):
        with torch.no_grad():
            # if message is old do not process it
            if msg1.header.stamp < rospy.Time.now() - rospy.Duration(1.):
                rospy.logdebug('Old image message received, skipping')
                return
            t0 = time()

            msgs = [msg1, msg2, msg3, msg4]

            # estimate image statistics if not provided
            if self.img_mean is None or self.img_std is None:
                rospy.logwarn('Image mean and std not provided. Estimating from data.')
                imgs = [self.cv_bridge.compressed_imgmsg_to_cv2(msg) for msg in msgs]
                imgs = np.stack(imgs)
                self.img_mean = imgs.mean(axis=(0, 1, 2))
                self.img_std = imgs.std(axis=(0, 1, 2))
                rospy.logwarn('Image mean: %s' % str(self.img_mean))
                rospy.logwarn('Image std: %s' % str(self.img_std))

            imgs = {}
            post_rots = {}
            post_trans = {}
            for msg in msgs:
                img = self.cv_bridge.compressed_imgmsg_to_cv2(msg)
                img, post_rot, post_tran = self.preprocess_img(img)
                imgs[msg.header.frame_id] = img
                post_rots[msg.header.frame_id] = post_rot
                post_trans[msg.header.frame_id] = post_tran

            # to ordered dicts
            imgs = OrderedDict(sorted(imgs.items()))
            post_rots = OrderedDict(sorted(post_rots.items()))
            post_trans = OrderedDict(sorted(post_trans.items()))
            intrins = OrderedDict(sorted(self.cams_intrins.items()))
            cams_to_hm = OrderedDict(sorted(self.cams_to_hm.items()))

            # to tensors
            imgs = torch.stack(list(imgs.values()))
            post_rots = torch.stack(list(post_rots.values()))
            post_trans = torch.stack(list(post_trans.values()))
            intrins = torch.stack(list(intrins.values()))
            cams_to_hm = torch.stack(list(cams_to_hm.values()))
            rots, trans = cams_to_hm[:, :3, :3], cams_to_hm[:, :3, 3]

            rospy.logdebug('Image preprocessing took %.3f' % (time() - t0))
            rospy.logdebug('Preprocessed image shape: %s' % str(imgs.shape))

            pred = self.model(imgs.unsqueeze(0).to(self.cfg.device),
                              rots.unsqueeze(0).to(self.cfg.device),
                              trans.unsqueeze(0).to(self.cfg.device),
                              intrins.unsqueeze(0).to(self.cfg.device),
                              post_rots.unsqueeze(0).to(self.cfg.device),
                              post_trans.unsqueeze(0).to(self.cfg.device),
                              )
            height = pred.squeeze().cpu().numpy()
            rospy.logdebug('Predicted height map shape: %s' % str(height.shape))
            rospy.logdebug('LSS inference time: %.3f' % (time() - t0))

            # publish height map as image
            height_uint8 = np.asarray(255 * normalize(height), dtype='uint8')
            img_msg = self.cv_bridge.cv2_to_imgmsg(height_uint8, encoding='mono8')
            img_msg.header.stamp = msg.header.stamp
            img_msg.header.frame_id = self.height_map_frame
            self.hm_img_pub.publish(img_msg)

            # # predict path
            # self.predict_paths(height, linear_vels=self.linear_vels, angular_vels=self.angular_vels)

            # publish height map as point cloud
            t1 = time()
            cloud_msg = height_map_to_point_cloud_msg(height, self.cfg.grid_res,
                                                      xyz=np.array([0., 0., 0.]), q=np.array([0., 0., 0., 1.]))
            cloud_msg.header.stamp = msg.header.stamp
            cloud_msg.header.frame_id = self.height_map_frame
            self.hm_cloud_pub.publish(cloud_msg)

            # publish height map as grid map
            grid_msg = height_map_to_gridmap_msg(height, self.cfg.grid_res,
                                                 xyz=np.array([0., 0., 0.]), q=np.array([0., 0., 0., 1.]))
            grid_msg.info.header.stamp = msg.header.stamp
            grid_msg.info.header.frame_id = self.height_map_frame
            self.hm_grid_pub.publish(grid_msg)
            rospy.logdebug('Height map publishing took %.3f' % (time() - t1))

    def sim(self, height, controls):
        assert isinstance(height, np.ndarray)
        assert height.shape[0] == height.shape[1]
        assert isinstance(controls, dict)
        assert 'stamps' in controls.keys()
        assert 'linear_v' in controls.keys()
        assert 'angular_w' in controls.keys()

        h, w = np.asarray(height.shape) * self.cfg.grid_res
        state = State(xyz=torch.tensor([0, 0., 0.], device=self.cfg.device).view(3, 1),
                   rot=torch.eye(3, device=self.cfg.device),
                   vel=torch.tensor([0., 0., 0.], device=self.cfg.device).view(3, 1),
                   omega=torch.tensor([0., 0., 0.], device=self.cfg.device).view(3, 1),
                   device=self.cfg.device)
        state[0][0] = -h / 2.  # move robot to the edge of the height map

        """ Create robot-terrain interaction models """
        system = RigidBodySoftTerrain(height=height,
                                      grid_res=self.cfg.grid_res,
                                      friction=self.cfg.friction,
                                      mass=self.cfg.robot_mass,
                                      state=state,
                                      device=self.cfg.device, use_ode=False,
                                      interaction_model='diffdrive')

        # put models with their params to self.cfg.device
        system = system.to(self.cfg.device)
        tt = controls['stamps'].to(self.cfg.device)

        """ Navigation loop """
        dt = (tt[1:] - tt[:-1]).mean()

        xyz, Rs, linear_v, angular_w, forces = state
        xyz, Rs, linear_v, angular_w, forces = [xyz], [Rs], [linear_v], [angular_w], [forces]

        for t in range(len(tt[1:])):
            v, w = controls['linear_v'][t], controls['angular_w'][t]

            state[2][0] = v
            state[3][2] = w

            dstate = system.forward(t, state)
            state = state.update(dstate, dt)

            xyz.append(state[0])
            Rs.append(state[1])
            linear_v.append(state[2])
            angular_w.append(state[3])
            forces.append(state[4])

        # height map origin is at the edge of the map
        xyz = torch.stack(xyz) + torch.tensor([h / 2., 0., 0.], device=self.cfg.device).view(3, 1)
        Rs = torch.stack(Rs)
        linear_v = torch.stack(linear_v)
        angular_w = torch.stack(angular_w)
        forces = torch.stack(forces)

        states = [xyz, Rs, linear_v, angular_w, forces]

        return states

    def path_cost(self, states):
        assert isinstance(states, list)
        assert len(states) == 5
        xyz, Rs, linear_v, angular_w, forces = states
        # path cost as a sum of force magnitudes
        assert forces.ndim == 3  # (n_samples, 3, n_points)
        # reduce forces acting on all robot points
        forces = forces.sum(dim=2)
        assert forces.shape[1] == 3
        cost = forces.norm(dim=1).mean()
        return cost

    def predict_paths(self, height, linear_vels=None, angular_vels=None):
        if angular_vels is None:
            angular_vels = [0.]
        if linear_vels is None:
            linear_vels = [1.]
        assert isinstance(height, np.ndarray)

        tt = torch.linspace(0., self.cfg.total_sim_time, self.cfg.n_samples)
        # paths marker array
        marker_array = MarkerArray()
        path_id = 0
        lower_cost_poses = None
        max_path_cost = torch.tensor(-np.inf, device=self.cfg.device)
        min_path_cost = torch.tensor(np.inf, device=self.cfg.device)
        for v in linear_vels:
            for w in angular_vels:
                # controls
                controls = {
                    'stamps': tt,
                    'linear_v': v * torch.ones(self.cfg.n_samples),
                    'angular_w': w * torch.ones(self.cfg.n_samples)
                }

                # predict states
                t0 = time()
                states = self.sim(height, controls)
                t1 = time()
                rospy.logdebug('Path of %d samples simulation took %.3f' % (self.cfg.n_samples, t1 - t0))

                # create path message (Marker)
                xyz = states[0].cpu().numpy()[::100]
                Rs = states[1].cpu().numpy()[::100]
                Ts = np.zeros((len(xyz), 4, 4))
                Ts[:, :3, :3] = Rs
                Ts[:, :3, 3:4] = xyz
                Ts[:, 3, 3] = 1.

                # compute path cost
                path_cost = self.path_cost(states)
                # rospy.logdebug('Path cost: %.3f' % path_cost.item())
                if path_cost > max_path_cost:
                    max_path_cost = path_cost.clone()
                if path_cost < min_path_cost:
                    min_path_cost = path_cost.clone()
                    lower_cost_poses = Ts
                # normalize path cost
                path_cost = (path_cost - min_path_cost) / (max_path_cost - min_path_cost) if max_path_cost > min_path_cost else path_cost
                # rospy.logdebug('Path cost normalized: %.3f' % path_cost.item())

                # map path cost to color (lower cost -> greener, higher cost -> redder)
                color = np.array([0., 1., 0.]) + (np.array([1., 0., 0.]) - np.array([0., 1., 0.])) * path_cost.item()
                marker_msg = to_marker(Ts, color=color)
                marker_msg.header.stamp = rospy.Time.now()
                marker_msg.header.frame_id = self.height_map_frame
                marker_msg.ns = 'paths'
                marker_msg.id = path_id
                path_id += 1
                marker_array.markers.append(marker_msg)
                rospy.logdebug('Path to marker array conversion took %.3f' % (time() - t1))

        # publish all sampled paths
        self.paths_pub.publish(marker_array)
        # publish lower cost path
        if lower_cost_poses is not None:
            path_msg = to_path(lower_cost_poses, stamp=rospy.Time.now(), frame_id=self.height_map_frame)
            path_msg.header.stamp = rospy.Time.now()
            path_msg.header.frame_id = self.height_map_frame
            self.lc_path_pub.publish(path_msg)


def main():
    rospy.init_node('lss', anonymous=True, log_level=rospy.DEBUG)

    cfg = Config()
    cfg.grid_res = 0.1
    cfg.device = 'cuda'
    cfg.d_max = 6.4
    cfg.d_min = 0.6
    cfg.total_sim_time = rospy.get_param('~total_sim_time')
    cfg.n_samples = 100 * int(cfg.total_sim_time)

    grid_conf = {
        'xbound': [-cfg.d_max, cfg.d_max, cfg.grid_res],
        'ybound': [-cfg.d_max, cfg.d_max, cfg.grid_res],
        'zbound': [-2.0, 2.0, 4.0],
        'dbound': [cfg.d_min, cfg.d_max, cfg.grid_res],
    }

    data_aug_conf = {
        'resize_lim': (0.193, 0.225),
        'final_dim': (128, 352),
        'rot_lim': (-5.4, 5.4),
        'H': 1200, 'W': 1920,
        'rand_flip': False,
        'bot_pct_lim': (0.0, 0.22),
        'cams': ['CAM_FRONT', 'CAM_LEFT', 'CAM_REAR', 'CAM_RIGHT'],
        'Ncams': 4,
    }

    img_topics = rospy.get_param('~img_topics')
    camera_info_topics = rospy.get_param('~camera_info_topics')
    height_map_frame = rospy.get_param('~height_map_frame')
    # model weights
    model_weights = rospy.get_param('~model_weights', os.path.join(pkg_path, 'config/weights/lss/lss_train9.pt'))
    # control parameters
    # linear_vels = rospy.get_param('~linear_vels')
    # angular_vels = rospy.get_param('~angular_vels')

    try:
        node = LSS(cfg=cfg, data_aug_conf=data_aug_conf, grid_conf=grid_conf,
                   model_weights=model_weights,
                   img_topics=img_topics, camera_info_topics=camera_info_topics,
                   height_map_frame=height_map_frame,
                   # linear_vels=linear_vels, angular_vels=angular_vels
                   )
        rospy.spin()
    except rospy.ROSInterruptException:
        pass


if __name__ == '__main__':
    main()
