#!/usr/bin/env python

import os
import torch
import numpy as np
from torch.utils.data import ConcatDataset
from monoforce.models.lss.model import compile_model
from monoforce.datasets.data import OmniDemData, explore_data
from monoforce.config import Config
from monoforce.datasets import seq_paths
from monoforce.models.lss.tools import MSELoss, WeightedMSELoss
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter


torch.set_default_dtype(torch.float32)


def create_datasets(train_paths, val_path, data_aug_conf, grid_conf, cfg, debug=False, vis=False):
    # create dataset for LSS model training
    datasets = []
    print('Train paths:', train_paths)
    print('Val path:', val_path)
    for path in train_paths:
        assert os.path.exists(path)
        train_ds = OmniDemData(path, is_train=True, data_aug_conf=data_aug_conf, cfg=cfg)
        # print(f'Train dataset from path {path} size is {len(train_ds)}')
        if vis:
            explore_data(path, grid_conf, data_aug_conf, cfg, save=False)
        datasets.append(train_ds)
    train_ds = ConcatDataset(datasets)

    # create validation dataset
    val_ds = OmniDemData(val_path, is_train=False, data_aug_conf=data_aug_conf, cfg=cfg)
    if vis:
        explore_data(val_path, grid_conf, data_aug_conf, cfg, save=False)

    if debug:
        print('Debug mode: using small datasets')
        train_ds = torch.utils.data.Subset(train_ds, np.random.choice(len(train_ds), 32, replace=False))
        val_ds = torch.utils.data.Subset(val_ds, np.random.choice(len(val_ds), 16, replace=False))
    print('Training dataset size:', len(train_ds))
    print('Validation dataset size:', len(val_ds))

    return train_ds, val_ds


def main():
    cfg = Config()
    cfg.d_min = 0.6
    cfg.d_max = 6.4
    cfg.grid_res = 0.1
    cfg.h_max = 1.0
    cfg.device = torch.device('cuda:0')
    cfg.lr = 1e-3
    cfg.weight_decay = 1e-7
    cfg.hm_interp_method = None

    max_grad_norm = 5.0
    bsz = 4
    nworkers = 10
    nepochs = 100

    grid_conf = {
        'xbound': [-cfg.d_max, cfg.d_max, cfg.grid_res],
        'ybound': [-cfg.d_max, cfg.d_max, cfg.grid_res],
        'zbound': [-10.0, 10.0, 20.0],
        'dbound': [cfg.d_min, cfg.d_max, cfg.grid_res],
    }

    data_aug_conf = {
                    'resize_lim': (0.193, 0.225),
                    'final_dim': (128, 352),
                    'rot_lim': (-5.4, 5.4),
                    'H': 1200, 'W': 1920,
                    'rand_flip': False,
                    'bot_pct_lim': (0.0, 0.0),
                    'cams': ['CAM_FRONT', 'CAM_REAR', 'CAM_RIGHT', 'CAM_LEFT'],
                    'Ncams': 4,
                }

    # train_ds_paths = seq_paths[:-1]
    # val_ds_path = seq_paths[-1]
    train_ds_paths = seq_paths[:2]
    val_ds_path = seq_paths[2]

    train_ds, val_ds = create_datasets(train_ds_paths, val_ds_path,
                                       data_aug_conf=data_aug_conf, grid_conf=grid_conf, cfg=cfg,
                                       vis=False, debug=False)
    trainloader = torch.utils.data.DataLoader(train_ds, batch_size=bsz, shuffle=True, num_workers=nworkers)
    valloader = torch.utils.data.DataLoader(val_ds, batch_size=bsz, shuffle=False, num_workers=nworkers)

    model = compile_model(grid_conf, data_aug_conf, outC=1)
    model.to(cfg.device)

    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)
    loss_fn = WeightedMSELoss()

    writer = SummaryWriter(log_dir='../config/tb_runs/lss/')

    min_loss = np.inf
    min_train_loss = np.inf
    train_counter = 0
    val_counter = 0
    for epoch in range(nepochs):
        # training epoch
        model.train()
        train_loss = 0.0
        for batchi, (imgs, rots, trans, intrins, post_rots, post_trans, heightmap)\
                in tqdm(enumerate(trainloader), total=len(trainloader)):
            optimizer.zero_grad()

            inputs = [imgs, rots, trans, intrins, post_rots, post_trans]
            inputs = [torch.as_tensor(i, dtype=torch.float32) for i in inputs]
            inputs = [i.to(cfg.device) for i in inputs]
            preds = model(*inputs)

            heightmap = torch.as_tensor(heightmap, dtype=torch.float32)
            heightmap = heightmap.to(cfg.device)
            B, D, H, W = heightmap.shape
            height, weights = heightmap[:, 0].view(B, 1, H, W), heightmap[:, 1].view(B, 1, H, W)

            loss = loss_fn(preds, height, weights)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            optimizer.step()
            train_loss += loss.item()

            train_counter += 1
            writer.add_scalar('train/loss', loss, train_counter)

        train_loss /= len(trainloader)
        print('Epoch:', epoch, 'Train loss:', train_loss)
        writer.add_scalar('train/epoch_loss', train_loss, epoch)
        if train_loss < min_train_loss:
            min_train_loss = train_loss
            print('Saving train model...')
            torch.save(model.state_dict(), f'train_lss.pt')

        # validation epoch
        with torch.no_grad():
            val_loss = 0.0
            for batchi, (imgs, rots, trans, intrins, post_rots, post_trans, heightmap)\
                    in tqdm(enumerate(valloader), total=len(valloader)):
                inputs = [imgs, rots, trans, intrins, post_rots, post_trans]
                inputs = [torch.as_tensor(i, dtype=torch.float32) for i in inputs]
                inputs = [i.to(cfg.device) for i in inputs]
                preds = model(*inputs)

                heightmap = torch.as_tensor(heightmap, dtype=torch.float32)
                heightmap = heightmap.to(cfg.device)
                B, D, H, W = heightmap.shape
                height, weights = heightmap[:, 0].view(B, 1, H, W), heightmap[:, 1].view(B, 1, H, W)

                loss = loss_fn(preds, height, weights)
                val_loss += loss.item()

                val_counter += 1
                writer.add_scalar('val/loss', loss, val_counter)

            val_loss /= len(valloader)
            print('Epoch:', epoch, 'Validation loss:', val_loss)
            writer.add_scalar('val/epoch_loss', val_loss, epoch)

            if val_loss < min_loss:
                min_loss = val_loss
                model.eval()
                print('Saving model...')
                torch.save(model.state_dict(), 'lss.pt')
                model.train()


if __name__ == '__main__':
    main()
