#!/usr/bin/env python

import os
import matplotlib.pyplot as plt
import matplotlib as mpl
from mayavi import mlab
from tqdm import tqdm
import numpy as np
import torch
from torch.utils.data import ConcatDataset
from monoforce.models import RigidBodySoftTerrain, State
from monoforce.models.lss.tools import denormalize_img, ego_to_cam, get_only_in_img_mask
from monoforce.models.lss.model import compile_model
from monoforce.datasets.data import TravData, explore_data
from monoforce.config import Config
from monoforce.datasets import robingas_husky_seq_paths, oru_seq_paths
from monoforce.utils import read_yaml
from monoforce.control import pose_control
from monoforce.losses import translation_difference, rotation_difference
from monoforce.transformations import rot2rpy
from monoforce.vis import draw_coord_frames, animate_trajectory, setup_visualization


class Evaluator:
    def __init__(self, dphys_config_path, lss_config_path, data_paths, model_path=None):
        self.model_path = model_path
        assert model_path is None or os.path.isfile(model_path), 'Model file %s does not exist' % model_path
        self.config_path = dphys_config_path
        assert os.path.isfile(dphys_config_path), 'Config file %s does not exist' % dphys_config_path
        self.lss_config_path = lss_config_path
        assert os.path.isfile(lss_config_path), 'LSS config file %s does not exist' % lss_config_path
        self.data_paths = data_paths
        for path in self.data_paths:
            assert os.path.isdir(path), 'Data path %s does not exist' % path

        self.cfg = Config()
        self.cfg.from_yaml(dphys_config_path)

        self.lss_config = read_yaml(lss_config_path)
        self.grid_conf = self.lss_config['grid_conf']
        self.data_aug_conf = self.lss_config['data_aug_conf']

        self.model = self.load_model()

        self.loss_fn = torch.nn.MSELoss()

        self.metrics = {'trans_diff': [],
                        'rot_diff': [],
                        'flip_over_counts': 0.}

    def load_model(self):
        return None

    def hm_to_imgs_projection(self, seq_path):
        ds = TravData(seq_path, is_train=False, data_aug_conf=self.data_aug_conf, cfg=self.cfg)

        fig = plt.figure(figsize=(20, 10))
        gs = mpl.gridspec.GridSpec(3, 6)
        gs.update(wspace=0.0, hspace=0.0, left=0.0, right=1.0, top=1.0, bottom=0.0)
        with torch.no_grad():
            for i in tqdm(range(len(ds)), total=len(ds)):
                imgs, rots, trans, intrins, post_rots, post_trans, hm_lidar, hm_traj, map_pose = ds[i]
                height = self.get_height(i, ds)

                # clear grid axes
                plt.clf()

                # create height map grid
                z_grid = height
                x_grid = torch.arange(-self.cfg.d_max, self.cfg.d_max, self.cfg.grid_res)
                y_grid = torch.arange(-self.cfg.d_max, self.cfg.d_max, self.cfg.grid_res)
                x_grid, y_grid = torch.meshgrid(x_grid, y_grid)
                hm_points = torch.stack([x_grid, y_grid, z_grid], dim=-1)
                hm_points = hm_points.view(-1, 3).T

                # plot images and height map points
                for img_gs_ids, imgi in zip([(1, 2), (0, 1), (1, 0), (2, 1)], range(imgs.shape[1])):
                    i, j = img_gs_ids
                    ax = plt.subplot(gs[i, j])
                    img = imgs[imgi]
                    img = denormalize_img(img)
                    # project height map points to image
                    ego_pts = ego_to_cam(hm_points, rots[imgi], trans[imgi], intrins[imgi])
                    img_H, img_W = self.data_aug_conf['H'], self.data_aug_conf['W']
                    mask = get_only_in_img_mask(ego_pts, img_H, img_W)
                    plot_pts = post_rots[imgi].matmul(ego_pts) + post_trans[imgi].unsqueeze(1)
                    ax.imshow(img)
                    ax.scatter(plot_pts[0, mask], plot_pts[1, mask], s=0.5, c=hm_points[2, mask],
                               cmap='jet', vmin=-1.0, vmax=1.0)
                    ax.axis('off')

                # plot prediction as image
                ax = plt.subplot(gs[1, 1])
                ax.imshow(z_grid.T, cmap='jet', vmin=-1.0, vmax=1.0, origin='lower')

                # plot prediction as surface
                ax = plt.subplot(gs[:, 3:6], projection='3d')
                ax.plot_surface(x_grid, y_grid, z_grid, cmap='jet', vmin=-1.0, vmax=1.0)
                ax.set_zlim(-2.0, 2.0)
                # set up view
                ax.view_init(elev=60., azim=-80.)
                ax.set_xlabel('x [m]')
                ax.set_ylabel('y [m]')
                ax.set_zlabel('z [m]')

                plt.pause(1.)
                plt.draw()

    def compute_losses(self, batch):
        loss_geom, loss_rigid = 0.0, np.nan
        return loss_geom, loss_rigid

    def eval_terrain_encoder(self, bsz=1, nworkers=10):
        all_seqs_mean_geom_loss, all_seqs_mean_rigid_loss = 0.0, 0.0
        for path in self.data_paths:
            print(f'Evaluation on {os.path.basename(path)}...')
            # explore_data(path, self.grid_conf, self.data_aug_conf, self.cfg, self.model_path, save=False)

            val_ds = TravData(path, is_train=False, data_aug_conf=self.data_aug_conf, cfg=self.cfg)
            valloader = torch.utils.data.DataLoader(val_ds, batch_size=bsz, shuffle=False, num_workers=nworkers)

            # validation epoch
            with torch.no_grad():
                eval_geom_loss, eval_rigid_loss = 0.0, 0.0
                for batch in tqdm(valloader, total=len(valloader)):
                    loss_geom, loss_rigid = self.compute_losses(batch)

                    eval_geom_loss = eval_geom_loss + loss_geom
                    eval_rigid_loss = eval_rigid_loss + loss_rigid
                eval_geom_loss /= len(valloader)
                eval_rigid_loss /= len(valloader)

            print(f'For seq {os.path.basename(path)} mean geom loss: {eval_geom_loss:.3f}, '
                  f'mean rigid loss: {eval_rigid_loss:.3f}')
            all_seqs_mean_geom_loss += eval_geom_loss
            all_seqs_mean_rigid_loss += eval_rigid_loss
        all_seqs_mean_geom_loss /= len(self.data_paths)
        all_seqs_mean_rigid_loss /= len(self.data_paths)
        print(f'Average evaluation geom loss: {all_seqs_mean_geom_loss:.3f}, '
              f'average evaluation rigid loss: {all_seqs_mean_rigid_loss:.3f}')

        return all_seqs_mean_geom_loss, all_seqs_mean_rigid_loss

    def get_height(self, i: int, ds: TravData):
        sample = ds[i]
        imgs, rots, trans, intrins, post_rots, post_trans, hm_lidar, hm_traj, map_pose = sample
        height = hm_lidar[0]
        return height
    
    def get_states(self, i: int, ds: TravData):
        traj = ds.get_traj(i)
        poses = traj['poses']
        # transform poses to the same coordinate frame as the height map
        Tr = np.linalg.inv(poses[0])
        poses = np.asarray([np.matmul(Tr, p) for p in poses])
        # count time from 0
        tstamps = traj['stamps']
        tstamps = tstamps - tstamps[0]

        poses = np.asarray(poses, dtype=np.float32)
        tstamps = np.asarray(tstamps, dtype=np.float32)

        xyz = torch.as_tensor(poses[:, :3, 3])
        rot = torch.as_tensor(poses[:, :3, :3])

        n_states = len(xyz)
        tt = torch.tensor(tstamps)[None].T

        dps = torch.diff(xyz, dim=0)
        dt = torch.diff(tt, dim=0)
        theta = torch.atan2(dps[:, 1], dps[:, 0]).view(-1, 1)
        theta = torch.cat([theta[:1], theta], dim=0)

        vel = torch.zeros_like(xyz)
        vel[:-1] = dps / dt
        omega = torch.zeros_like(xyz)
        omega[:-1, 2:3] = torch.diff(theta, dim=0) / dt  # + torch.diff(angles, dim=0)[:, 2:3] / dt

        forces = torch.zeros((n_states, 3, 10))
        states = (xyz.view(n_states, 3, 1),
                  rot.view(n_states, 3, 3),
                  vel.view(n_states, 3, 1),
                  omega.view(n_states, 3, 1),
                  forces.view(n_states, 3, 10))
        states = tuple([s.to(self.cfg.device) for s in states])
        return states


    def get_data(self, i: int, ds: TravData):
        """
        Get ground truth data sample from the dataset

        :param i: index of the sample
        :param ds: dataset of type TravData

        :return: states_true, height
        """
        assert i < len(ds), 'Index out of range'
        assert isinstance(ds, TravData), 'Invalid dataset type'

        states_true = self.get_states(i, ds)

        # get height map
        height = self.get_height(i, ds)

        return states_true, height

    def eval_diff_physics(self, height, states_true, vis=False):
        """
        Simulate the system with P control.
        Robot visits a set of waypoints.
        Diff-drive (controlled with X-linear and Z-angular velocities) robot motion model is used.
        """
        xyz_true, rot_true, vel_true, omega_true, forces_true = states_true
        n_true_states = len(xyz_true)

        """ Create robot-terrain interaction models """
        system = RigidBodySoftTerrain(height=torch.as_tensor(height),
                                      grid_res=self.cfg.grid_res,
                                      friction=self.cfg.friction, mass=self.cfg.robot_mass,
                                      state=State(
                                          xyz=xyz_true[0] + torch.tensor([0., 0., 1.], device=self.cfg.device).view(xyz_true[0].shape),
                                          rot=rot_true[0],
                                          vel=vel_true[0],
                                          omega=omega_true[0],
                                          device=self.cfg.device),
                                      device=self.cfg.device, use_ode=False,
                                      interaction_model='diffdrive')

        # put models with their params to self.cfg.device
        system = system.to(self.cfg.device)
        s0 = system.state
        tt = torch.linspace(0, self.cfg.total_sim_time, self.cfg.n_samples).to(self.cfg.device)

        if vis:
            states = system.sim(s0, tt)
            """ Set-up visualization """
            vis_cfg = setup_visualization(system=system,
                                          states=states,
                                          states_true=states_true,
                                          cfg=self.cfg)

        """ Navigation loop """
        state = system.state
        states = []
        dt = (tt[1:] - tt[:-1]).mean()
        loss_trans_sum = torch.tensor(0., device=self.cfg.device)
        loss_rot_sum = torch.tensor(0., device=self.cfg.device)
        poses_eval = []
        for i in range(n_true_states - 1):
            # print('Going from pose %s -> to waypoint %s' % (state[0].squeeze(), xyz_true[i + 1].squeeze()))
            time_interval = tt[i * self.cfg.n_samples // (n_true_states - 1):(i + 1) * self.cfg.n_samples // (
                        n_true_states - 1)]

            pos_x, pos_R, vel_x, vel_omega, forces = state
            pos_x, pos_R, vel_x, vel_omega, forces = [pos_x], [pos_R], [vel_x], [vel_omega], [forces]

            roll, pitch, yaw = rot2rpy(pos_R[-1].squeeze())

            if torch.abs(roll) > np.pi / 2. or torch.abs(pitch) > np.pi / 2.:
                print('Robot is upside down, skipping evaluation')
                return None, None

            goal_pose = torch.eye(4, device=self.cfg.device)
            goal_pose[:3, 3:4] = xyz_true[i + 1]
            goal_pose[:3, :3] = rot_true[i + 1]

            for t in time_interval[1:]:
                v, w = pose_control(state, goal_pose, allow_backwards=True,
                                    Kp_rho=2., Kp_theta=4., Kp_yaw=4., dist_reached=0.01)
                state[2][0] = v
                state[3][2] = w

                dstate = system.forward(t, state)
                state = state.update(dstate, dt)

                pos_x.append(state[0])
                pos_R.append(state[1])
                vel_x.append(state[2])
                vel_omega.append(state[3])
                forces.append(state[4])
            # print('Reached waypoint with accuracy: %.2f [m]' % dist.item())

            states_interval = [torch.stack(pos_x), torch.stack(pos_R), torch.stack(vel_x), torch.stack(vel_omega),
                               torch.stack(forces)]
            states.append(states_interval)

            # compute loss
            loss_trans = translation_difference(pos_x[-1].view(1, 3, 1), states_true[0][i + 1].view(1, 3, 1))
            loss_rot = rotation_difference(pos_R[-1].view(1, 3, 3), states_true[1][i + 1].view(1, 3, 3))

            loss_trans_sum += loss_trans
            loss_rot_sum += loss_rot

            # log poses at the end of each interval for which we compute loss
            pose_eval = torch.eye(4)
            pose_eval[:3, 3:4] = pos_x[-1].view(3, 1)
            pose_eval[:3, :3] = pos_R[-1].view(3, 3)
            poses_eval.append(pose_eval)

        pos_x = torch.cat([x[0] for x in states], dim=0)
        pos_R = torch.cat([x[1] for x in states], dim=0)
        vel_x = torch.cat([x[2] for x in states], dim=0)
        vel_omega = torch.cat([x[3] for x in states], dim=0)
        forces = torch.cat([x[4] for x in states], dim=0)

        states = (pos_x, pos_R, vel_x, vel_omega, forces)

        loss_trans_sum /= (n_true_states - 1)
        loss_rot_sum /= (n_true_states - 1)

        # visualize trajectory
        if vis:
            system.update_trajectory(states=states)
            draw_coord_frames(torch.stack(poses_eval).cpu().numpy(), scale=0.1)
            animate_trajectory(system, vis_cfg)
            mlab.show()

        return loss_trans_sum, loss_rot_sum

    def evaluate(self, vis=False):
        for path in self.data_paths:
            print(f'Evaluation on {os.path.basename(path)}...')
            ds = TravData(path, is_train=False, data_aug_conf=self.data_aug_conf, cfg=self.cfg)
            for i in tqdm(range(len(ds))):
                states_true, height = self.get_data(i, ds)
                with torch.no_grad():
                    trans_diff, rot_diff = self.eval_diff_physics(height, states_true, vis=vis)
                    if rot_diff is not None:
                        # print('Loss translation at waypoints: %.2f [m]' % trans_diff)
                        # print('Loss rotation at waypoints: %.2f [deg]' % (rot_diff * 180 / np.pi))
                        self.metrics['trans_diff'].append(trans_diff.item())
                        self.metrics['rot_diff'].append(rot_diff.item())
                    else:
                        self.metrics['flip_over_counts'] += 1
                # break
            # break

        trans_diff_final = np.mean(self.metrics['trans_diff'])
        rot_diff_final = np.mean(self.metrics['rot_diff'])
        flip_over_rate_final = self.metrics['flip_over_counts'] / (
                    len(self.metrics['trans_diff']) + self.metrics['flip_over_counts'])

        print(f'Average loss translation: %.2f [m]' % trans_diff_final)
        print(f'Average loss rotation: %.2f [deg]' % (rot_diff_final * 180 / np.pi))
        print(f'Flip over rate: %.2f' % flip_over_rate_final)


class EvaluatorLSS(Evaluator):
    def __init__(self, dphys_config_path, lss_config_path, data_paths,
                 model_path='../config/tb_runs/lss_2024_02_27_09_17_36/train_lss.pt'):
        super().__init__(dphys_config_path, lss_config_path, data_paths, model_path)

    def load_model(self):
        print('Loading LSS model')
        model = compile_model(self.grid_conf, self.data_aug_conf, outC=1)
        print('Loading', self.model_path)
        model.load_state_dict(torch.load(self.model_path))
        model.to(self.cfg.device)
        return model

    def get_height(self, i: int, ds: TravData):
        sample = ds[i]
        imgs, rots, trans, intrins, post_rots, post_trans, hm_lidar, hm_traj, map_pose = sample
        with torch.no_grad():
            inputs = [imgs, rots, trans, intrins, post_rots, post_trans]
            inputs = [torch.as_tensor(i[None], device=self.cfg.device) for i in inputs]
            height_pred = self.model(*inputs)
            height = height_pred.squeeze().cpu()
        return height

    def compute_losses(self, batch):
        batch = [torch.as_tensor(b, device=self.cfg.device) for b in batch]
        imgs, rots, trans, intrins, post_rots, post_trans, hm_lidar, hm_traj, map_pose = batch
        height_lidar, weights_lidar = hm_lidar[:, 0:1], hm_lidar[:, 1:2]
        height_traj, weights_traj = hm_traj[:, 0:1], hm_traj[:, 1:2]
        inputs = [imgs, rots, trans, intrins, post_rots, post_trans]

        # compute loss
        voxel_feats = self.model.get_voxels(*inputs)
        height_pred_geom, height_pred_diff = self.model.bevencode(voxel_feats)
        height_pred_rigid = height_pred_geom - height_pred_diff

        loss_geom = self.loss_fn(height_pred_geom[weights_lidar.bool()], height_lidar[weights_lidar.bool()])
        loss_rigid = self.loss_fn(height_pred_rigid[weights_traj.bool()], height_traj[weights_traj.bool()])
        return loss_geom, loss_rigid


class EvaluatorKKT(Evaluator):
    def __init__(self, dphys_config_path, lss_config_path, data_paths,
                 model_path='../../pose-consistency-KKT-loss/weights/network_weights_s2d'):
        super().__init__(dphys_config_path, lss_config_path, data_paths, model_path)

    def load_model(self):
        print('Loading KKT model')
        import sys
        sys.path.append('../../pose-consistency-KKT-loss/scripts/')
        import network_s2d
        model = network_s2d.Net()
        model.load_state_dict(torch.load(self.model_path))
        model.eval()
        model.to(self.cfg.device)
        return model

    def get_height(self, i: int, ds: TravData):
        sample = ds[i]
        imgs, rots, trans, intrins, post_rots, post_trans, hm_lidar, hm_traj, map_pose = sample
        with torch.no_grad():
            input = torch.as_tensor(hm_lidar[None], device=self.cfg.device)
            hm_pred = self.model(input)
            height = hm_pred.squeeze().cpu()[0]
        return height

    def compute_losses(self, batch):
        batch = [torch.as_tensor(b, device=self.cfg.device) for b in batch]
        imgs, rots, trans, intrins, post_rots, post_trans, hm_lidar, hm_traj, map_pose = batch
        height_lidar, weights_lidar = hm_lidar[:, 0:1], hm_lidar[:, 1:2]

        hm_pred = self.model(hm_lidar)
        height_pred_geom = hm_pred[:, 0:1]

        loss_geom = self.loss_fn(height_pred_geom[weights_lidar.bool()], height_lidar[weights_lidar.bool()])
        loss_rigid = np.nan
        return loss_geom, loss_rigid


def main():
    dphys_config_path = '../config/dphys_cfg.yaml'
    lss_config_path = '../config/lss_cfg.yaml'
    data_paths = robingas_husky_seq_paths
    # data_paths = oru_seq_paths
    vis = True

    # evals = [Evaluator, EvaluatorLSS, EvaluatorKKT]
    evals = [EvaluatorLSS]

    for Eval in evals:
        evaluator = Eval(dphys_config_path=dphys_config_path,
                         lss_config_path=lss_config_path,
                         data_paths=data_paths)
        # evaluator.eval_terrain_encoder(bsz=4)
        # evaluator.hm_to_imgs_projection(data_paths[2])
        evaluator.evaluate(vis=vis)


if __name__ == '__main__':
    main()
