#!/usr/bin/env python

import os
import torch
import numpy as np
from torch.utils.data import ConcatDataset
from monoforce.imgproc import destandardize_img
from monoforce.models.lss.model import compile_model
from monoforce.datasets.data import OmniDEMData, explore_data
from monoforce.config import Config
from monoforce.datasets import seq_paths, sim_seq_paths
from monoforce.losses import RMSE
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
import matplotlib.pyplot as plt

torch.set_default_dtype(torch.float32)

def create_datasets(paths, data_aug_conf, grid_conf, cfg, val_fraction=0.1, debug=False, vis=False):
    # create dataset for LSS model training
    datasets = []
    print('Data paths:', paths)
    for path in paths:
        assert os.path.exists(path)
        ds = OmniDEMData(path, is_train=True, data_aug_conf=data_aug_conf, cfg=cfg)
        # print(f'Train dataset from path {path} size is {len(ds)}')
        if vis:
            explore_data(path, grid_conf, data_aug_conf, cfg, save=False)
        datasets.append(ds)
    ds = ConcatDataset(datasets)

    # random split to get train and validation datasets
    val_ds_size = int(val_fraction * len(ds))
    train_ds_size = len(ds) - val_ds_size
    train_ds, val_ds = torch.utils.data.random_split(ds, [train_ds_size, val_ds_size])

    if debug:
        print('Debug mode: using small datasets')
        train_ds = torch.utils.data.Subset(train_ds, np.random.choice(len(train_ds), 32, replace=False))
        val_ds = torch.utils.data.Subset(val_ds, np.random.choice(len(val_ds), 8, replace=False))
    print('Training dataset size:', len(train_ds))
    print('Validation dataset size:', len(val_ds))

    return train_ds, val_ds


def epoch(model, loss_fn, loader, device, writer, counter, optimizer=None, train=True):
    if train:
        assert optimizer is not None
        model.train()

    epoch_loss = 0.0
    max_grad_norm = 5.0

    for batchi, (imgs, rots, trans, intrins, post_rots, post_trans, heightmap) \
            in tqdm(enumerate(loader), total=len(loader)):

        inputs = [imgs, rots, trans, intrins, post_rots, post_trans]
        inputs = [torch.as_tensor(i, dtype=torch.float32) for i in inputs]
        inputs = [i.to(device) for i in inputs]
        height_pred = model(*inputs)

        heightmap = torch.as_tensor(heightmap, dtype=torch.float32)
        heightmap = heightmap.to(device)
        B, D, H, W = heightmap.shape
        height_gt, weights = heightmap[:, 0].view(B, 1, H, W), heightmap[:, 1].view(B, 1, H, W)

        loss = loss_fn(height_pred[weights.bool()], height_gt[weights.bool()])

        if train:
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
            optimizer.step()
        epoch_loss += loss.item()

        counter += 1
        writer.add_scalar(f"{'train' if train else 'val'}/iter_loss", loss, counter)

    epoch_loss /= len(loader)

    return epoch_loss, counter


def vis_pred(loader, model, grid_conf, device):
    fig = plt.figure(figsize=(20, 10))
    ax1 = fig.add_subplot(231)
    ax2 = fig.add_subplot(232, projection='3d')
    ax3 = fig.add_subplot(233, projection='3d')
    ax4 = fig.add_subplot(234)
    ax5 = fig.add_subplot(235)
    ax6 = fig.add_subplot(236)

    # visualize training predictions
    with torch.no_grad():
        imgs, rots, trans, intrins, post_rots, post_trans, hm_gt = next(iter(loader))
        inputs = [imgs, rots, trans, intrins, post_rots, post_trans]
        inputs = [torch.as_tensor(i, dtype=torch.float32, device=device) for i in inputs]
        hm_gt = torch.as_tensor(hm_gt, dtype=torch.float32, device=device)
        height_pred = model(*inputs)

        for ax in [ax1, ax2, ax3, ax4, ax5, ax6]:
            ax.clear()

        # plot image
        img = imgs[0][0].permute(1, 2, 0).cpu().numpy()
        # use ImageNet mean and std
        img = destandardize_img(img, img_mean=np.array([0.485, 0.456, 0.406]), img_std=np.array([0.229, 0.224, 0.225]))
        ax1.imshow(img)

        # plot prediction as surface
        ax2.set_title('Pred Surface')
        height = height_pred[0][0].cpu().numpy()
        height_gt = hm_gt[0][0].cpu().numpy()
        mask = hm_gt[0][1].bool().cpu().numpy()
        x_grid = np.arange(grid_conf['xbound'][0], grid_conf['xbound'][1], grid_conf['xbound'][2])
        y_grid = np.arange(grid_conf['ybound'][0], grid_conf['ybound'][1], grid_conf['ybound'][2])
        x_grid, y_grid = np.meshgrid(x_grid, y_grid)
        ax2.plot_surface(x_grid, y_grid, height, cmap='jet', vmin=-1.0, vmax=1.0)
        ax2.set_zlim(-1.0, 1.0)
        ax2.set_xlabel('x [m]')
        ax2.set_ylabel('y [m]')
        ax2.set_zlabel('z [m]')

        # plot ground truth as surface
        ax3.set_title('GT Surface')
        ax3.plot_surface(x_grid, y_grid, height_gt, cmap='jet', vmin=-1.0, vmax=1.0)
        ax3.set_zlim(-1.0, 1.0)
        ax3.set_xlabel('x [m]')
        ax3.set_ylabel('y [m]')
        ax3.set_zlabel('z [m]')

        # plot prediction as image
        ax4.set_title('Prediction')
        ax4.imshow(height, cmap='jet', vmin=-1.0, vmax=1.0)

        ax5.set_title('Masked Prediction')
        height_vis = height_gt.copy()
        height_vis[mask] = height[mask]
        ax5.imshow(height_vis, cmap='jet', vmin=-1.0, vmax=1.0)

        ax6.set_title('Ground truth')
        ax6.imshow(height_gt, cmap='jet', vmin=-1.0, vmax=1.0)

        return fig

def main():
    cfg = Config()
    cfg.d_min = 0.6
    cfg.d_max = 6.4
    cfg.grid_res = 0.1
    cfg.h_max = 1.5
    cfg.device = torch.device('cuda:0')
    cfg.lr = 1e-3
    cfg.weight_decay = 1e-7
    cfg.hm_interp_method = None

    bsz = 4
    nworkers = 12
    nepochs = 500
    save_model = True

    grid_conf = {
        'xbound': [-cfg.d_max, cfg.d_max, cfg.grid_res],
        'ybound': [-cfg.d_max, cfg.d_max, cfg.grid_res],
        'zbound': [-10.0, 10.0, 20.0],
        'dbound': [cfg.d_min, cfg.d_max, cfg.grid_res],
    }

    data_aug_conf = {
                    'resize_lim': (0.193, 0.225),
                    'final_dim': (128, 352),
                    'rot_lim': (-5.4, 5.4),
                    'H': 1200, 'W': 1920,
                    'rand_flip': False,
                    'bot_pct_lim': (0.0, 0.0),
                    'cams': ['CAM_FRONT', 'CAM_REAR', 'CAM_RIGHT', 'CAM_LEFT'],
                    'Ncams': 4,
                }

    ds_paths = seq_paths[:3]
    train_ds, val_ds = create_datasets(ds_paths, data_aug_conf=data_aug_conf,
                                       grid_conf=grid_conf, cfg=cfg,
                                       vis=False, debug=False)
    trainloader = torch.utils.data.DataLoader(train_ds, batch_size=bsz, shuffle=True, num_workers=nworkers)
    valloader = torch.utils.data.DataLoader(val_ds, batch_size=bsz, shuffle=False, num_workers=nworkers)

    model = compile_model(grid_conf, data_aug_conf, outC=1)
    model.to(cfg.device)
    model.train()

    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)
    loss_fn = RMSE()

    log_dir = f'../config/tb_runs/lss_{datetime.now()}/'
    writer = SummaryWriter(log_dir=log_dir)

    min_loss = np.inf
    min_train_loss = np.inf
    train_counter = 0
    val_counter = 0

    for e in range(nepochs):
        # training epoch
        train_loss, train_counter = epoch(model=model, loss_fn=loss_fn, loader=trainloader,
                                          device=cfg.device, writer=writer, counter=train_counter,
                                          optimizer=optimizer, train=True)
        print('Epoch:', e, 'Train loss:', train_loss)
        writer.add_scalar('train/epoch_loss', train_loss, e)

        if save_model and train_loss < min_train_loss:
            min_train_loss = train_loss
            print('Saving train model...')
            torch.save(model.state_dict(), os.path.join(log_dir, 'train_lss.pt'))

            # visualize training predictions
            fig = vis_pred(trainloader, model, grid_conf, device=cfg.device)
            writer.add_figure('train/prediction', fig, e)

        # validation epoch
        with torch.no_grad():
            val_loss, val_counter = epoch(model=model, loss_fn=loss_fn, loader=valloader,
                                          device=cfg.device, writer=writer, counter=val_counter, train=False)
            print('Epoch:', e, 'Validation loss:', val_loss)
            writer.add_scalar('val/epoch_loss', val_loss, e)
            if save_model and val_loss < min_loss:
                min_loss = val_loss
                model.eval()
                print('Saving model...')
                torch.save(model.state_dict(), os.path.join(log_dir, 'lss.pt'))
                model.train()

                # visualize validation predictions
                fig = vis_pred(valloader, model, grid_conf, device=cfg.device)
                writer.add_figure('val/prediction', fig, e)


if __name__ == '__main__':
    main()
